{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b62d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from typing import List, Optional, Dict, Any, Protocol\n",
    "from llm.LLMParser import get_default_parser\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26fbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Backend Interface ----------\n",
    "class ChatBackend(Protocol):\n",
    "    def start(self,\n",
    "              system_instruction: Optional[str] = None,\n",
    "              history: Optional[List[Dict[str, Any]]] = None) -> None: ...\n",
    "    def send(self, message: str) -> str: ...\n",
    "    def history(self) -> List[Dict[str, Any]]: ...\n",
    "    def reset(self) -> None: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e09316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Azure OpenAI backend (azure-openai) ----------\n",
    "\n",
    "class AzureOpenAIBackend(ChatBackend):\n",
    "    \"\"\"\n",
    "    Requires: pip install openai>=1.0.0\n",
    "    Env:\n",
    "        AZURE_OPENAI_API_KEY       - API key for Azure OpenAI\n",
    "        AZURE_OPENAI_ENDPOINT      - Your Azure endpoint (e.g. https://myresource.openai.azure.com/)\n",
    "        AZURE_OPENAI_DEPLOYMENT    - Deployment name (e.g. \"testdelaycategory\")\n",
    "        AZURE_OPENAI_API_VERSION   - Optional, defaults to \"2024-12-01-preview\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 api_key: Optional[str] = None,\n",
    "                 endpoint: Optional[str] = None,\n",
    "                 deployment: Optional[str] = None,\n",
    "                 api_version: Optional[str] = None,\n",
    "                 generation_config: Optional[Dict[str, Any]] = None):\n",
    "        import os\n",
    "        from openai import AzureOpenAI\n",
    "\n",
    "        # --- Get configuration from environment or args ---\n",
    "        self._api_key = api_key or os.getenv(\"AZURE_OPENAI_API_KEY\") or os.getenv(\"AZURE_GPT_35_TURBO_API_KEY\")\n",
    "        self._endpoint = endpoint or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self._deployment = deployment or os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"testdelaycategory\")\n",
    "        self._api_version = api_version or os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "        if not self._api_key or not self._endpoint:\n",
    "            raise ValueError(\"Missing required Azure OpenAI configuration: API key or endpoint.\")\n",
    "\n",
    "#         --- Initialize client ---\n",
    "#    âœ… Correct client initialization (no 'proxies', 'headers', etc.)\n",
    "        self._client = AzureOpenAI(\n",
    "        api_version=self._api_version,\n",
    "        azure_endpoint=self._endpoint,\n",
    "        api_key=self._api_key\n",
    "        )\n",
    "        # self._client = OpenAI(\n",
    "        # api_key=self._api_key,\n",
    "        # base_url=f\"{self._endpoint}/openai/deployments/{self._deployment}\",\n",
    "        # default_headers={\"api-key\": self._api_key},\n",
    "        # )\n",
    "        # self._deployment = \"chat/completions\"\n",
    "\n",
    "\n",
    "        # --- Other settings ---\n",
    "        self._generation_config = generation_config or {\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "        self._chat_history: List[Dict[str, Any]] = []\n",
    "        self._system_instruction: Optional[str] = None\n",
    "\n",
    "    # ---------- Session Management ----------\n",
    "\n",
    "    def start(self,\n",
    "              system_instruction: Optional[str] = None,\n",
    "              history: Optional[List[Dict[str, Any]]] = None) -> None:\n",
    "        \"\"\"Initialize or reset a chat session with optional system instruction and history.\"\"\"\n",
    "        self._system_instruction = system_instruction\n",
    "        self._chat_history = []\n",
    "\n",
    "        if system_instruction:\n",
    "            self._chat_history.append({\"role\": \"system\", \"content\": system_instruction})\n",
    "\n",
    "        if history:\n",
    "            self._chat_history.extend(history)\n",
    "\n",
    "    def send(self, message: str) -> str:\n",
    "        \"\"\"Send a user message to Azure OpenAI and get the model's response.\"\"\"\n",
    "        if not self._chat_history:\n",
    "            self.start()\n",
    "\n",
    "        # Append user message\n",
    "        self._chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Generate response\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._deployment,\n",
    "            messages=self._chat_history,\n",
    "            **self._generation_config\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content\n",
    "        # Append assistant response to history\n",
    "        self._chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "    def history(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Return the full chat history.\"\"\"\n",
    "        return self._chat_history\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Clear chat history and reset context.\"\"\"\n",
    "        self._chat_history = []\n",
    "        self._system_instruction = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737ba04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_backend(provider: Optional[str] = None,\n",
    "                 *,\n",
    "                 deployment: Optional[str] = None,\n",
    "                 endpoint: Optional[str] = None,\n",
    "                 api_key: Optional[str] = None,\n",
    "                 **kwargs) -> ChatBackend:\n",
    "    \"\"\"\n",
    "    provider: 'gemini' | 'openai' | 'anthropic' (defaults to 'gemini')\n",
    "    model_name: overrides provider default\n",
    "    Also reads env: LLM_PROVIDER, LLM_MODEL\n",
    "    \"\"\"\n",
    "    p = (provider or os.getenv(\"LLM_PROVIDER\") or \"gemini\").lower()\n",
    "    deployment = deployment or os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"testdelaycategory\")\n",
    "    endpoint = endpoint or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    api_key = api_key or os.getenv(\"AZURE_OPENAI_API_KEY\") or os.getenv(\"AZURE_GPT_35_TURBO_API_KEY\")\n",
    "\n",
    "    if p == \"gemini\":\n",
    "        return GeminiBackend(model_name=os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\"), **kwargs)\n",
    "\n",
    "    if p in (\"openai\", \"azure\", \"azureopenai\"):\n",
    "        # Use the AzureOpenAIBackend we defined\n",
    "        return AzureOpenAIBackend(\n",
    "            api_key=api_key,\n",
    "            endpoint=endpoint,\n",
    "            deployment=deployment,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    if p == \"anthropic\":\n",
    "        return AnthropicBackend(model_name=os.getenv(\"ANTHROPIC_MODEL\", \"claude-3-5-sonnet-latest\"), **kwargs)\n",
    "\n",
    "    raise ValueError(f\"Unsupported provider: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277df547",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad556031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------ LLM Examples ------------------------\n",
      "\n",
      "------------------------ Example 1: Environmental & Health Risks ------------------------\n",
      "\n",
      "Query: Are there asbestos filings or air quality complaints near 45-10 21st Street in Queens?\n",
      "Router Result: {\n",
      "  \"categories\": [\n",
      "    \"Construction & Permitting\",\n",
      "    \"Environmental & Health Risks\"\n",
      "  ],\n",
      "  \"confidence\": 0.85,\n",
      "  \"address\": [\n",
      "    {\n",
      "      \"house_number\": \"45-10\",\n",
      "      \"street_name\": \"21st Street\",\n",
      "      \"borough\": \"Queens\",\n",
      "      \"raw\": \"45-10 21st Street\",\n",
      "      \"notes\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"dataset_names\": [\n",
      "    \"Asbestos Control Program\",\n",
      "    \"Clean Air Tracking System (CATS)\",\n",
      "    \"DOB Job filings\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('\\n------------------------ LLM Examples ------------------------')\n",
    "print(\"\\n------------------------ Example 1: Environmental & Health Risks ------------------------\")\n",
    "example_query = \"Are there asbestos filings or air quality complaints near 45-10 21st Street in Queens?\"\n",
    "llm_backend = make_backend(provider=\"openai\",deployment=openai_deployment_name,endpoint=azure_endpoint,api_key=azure_openai_api_key)\n",
    "parser = get_default_parser(backend=llm_backend)\n",
    "result = parser.route_query_to_datasets(example_query)\n",
    "print(\"\\nQuery:\", example_query)\n",
    "print(\"Router Result:\", json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c873ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41.0\n"
     ]
    }
   ],
   "source": [
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35302740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
